{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFcCHlu6evwHFQ5Z8j6LmV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jefffish09/MachineLearning/blob/dev/NER/lstm/lstm_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zy4FuH_q_w3"
      },
      "source": [
        "References:\r\n",
        "\r\n",
        "* https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\r\n",
        "* https://github.com/snehalnair/Named-Entity-Recognition\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "-ulJYcmCG3UY",
        "outputId": "6de83f04-cc57-4972-db3e-bedb5538a707"
      },
      "source": [
        "# Get the raw dataset\r\n",
        "\r\n",
        "!curl -s -LO https://github.com/MahmooudTaha/NLP-2019/raw/master/ner_dataset.csv.zip\r\n",
        "!unzip -o ner_dataset.csv.zip\r\n",
        "!rm -f ner_dataset.csv.zip\r\n",
        "\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from itertools import chain\r\n",
        "\r\n",
        "\r\n",
        "def get_dict_map(data, token_or_tag):\r\n",
        "    tok2idx = {}\r\n",
        "    idx2tok = {}\r\n",
        "    if token_or_tag == \"token\":\r\n",
        "        vocab = list(set(data[\"Word\"].to_list()))\r\n",
        "    else:\r\n",
        "        vocab = list(set(data[\"Tag\"].to_list()))\r\n",
        "    idx2tok = {idx: tok for idx, tok in enumerate(vocab)}\r\n",
        "    tok2idx = {tok: idx for idx, tok in enumerate(vocab)}\r\n",
        "    return tok2idx, idx2tok\r\n",
        "\r\n",
        "\r\n",
        "data = pd.read_csv(\"/content/ner_dataset.csv\", encoding=\"Windows-1252\")\r\n",
        "token2idx, idx2token = get_dict_map(data, \"token\")\r\n",
        "tag2idx, idx2tag = get_dict_map(data, \"tag\")\r\n",
        "\r\n",
        "data[\"Word_idx\"] = data[\"Word\"].map(token2idx)\r\n",
        "data[\"Tag_idx\"] = data[\"Tag\"].map(tag2idx)\r\n",
        "\r\n",
        "# Fill na\r\n",
        "data_fillna = data.fillna(method=\"ffill\", axis=0)\r\n",
        "\r\n",
        "# Groupby and collect columns\r\n",
        "data_group = data_fillna.groupby([\"Sentence #\"], as_index=False)[\"Word\", \"POS\", \"Tag\", \"Word_idx\", \"Tag_idx\"].agg(lambda x: list(x))\r\n",
        "\r\n",
        "# Visualise data\r\n",
        "data_group.head()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ner_dataset.csv.zip\n",
            "  inflating: ner_dataset.csv         \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "      <th>Word_idx</th>\n",
              "      <th>Tag_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
              "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
              "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
              "      <td>[29453, 24431, 972, 19273, 19658, 25560, 16911...</td>\n",
              "      <td>[3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 10</td>\n",
              "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
              "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
              "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
              "      <td>[2928, 25956, 18753, 23841, 33605, 10079, 1050...</td>\n",
              "      <td>[0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 100</td>\n",
              "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
              "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
              "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
              "      <td>[11750, 25537, 22242, 28605, 4287, 33319, 1730...</td>\n",
              "      <td>[3, 3, 13, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1000</td>\n",
              "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
              "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
              "      <td>[26972, 23959, 15738, 23407, 7888, 27521, 1347...</td>\n",
              "      <td>[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 10000</td>\n",
              "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
              "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
              "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
              "      <td>[5753, 34147, 11510, 2580, 2140, 24732, 25928,...</td>\n",
              "      <td>[5, 3, 3, 10, 2, 3, 13, 3, 5, 3, 0, 3, 0, 3, 3...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Sentence #  ...                                            Tag_idx\n",
              "0      Sentence: 1  ...  [3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, ...\n",
              "1     Sentence: 10  ...  [0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
              "2    Sentence: 100  ...  [3, 3, 13, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 15...\n",
              "3   Sentence: 1000  ...                  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
              "4  Sentence: 10000  ...  [5, 3, 3, 10, 2, 3, 13, 3, 5, 3, 0, 3, 0, 3, 3...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2RTuGAECJKt",
        "outputId": "b4db855f-0e11-4b0d-ba9f-9e613af3ea5d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "\r\n",
        "\r\n",
        "def get_pad_train_test_val(data_group, data):\r\n",
        "\r\n",
        "    # Get max token and tag length\r\n",
        "    n_token = len(list(set(data[\"Word\"].to_list())))\r\n",
        "    n_tag = len(list(set(data[\"Tag\"].to_list())))\r\n",
        "\r\n",
        "    # Pad tokens (X var)    \r\n",
        "    tokens = data_group[\"Word_idx\"].tolist()\r\n",
        "    maxlen = max([len(s) for s in tokens])\r\n",
        "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype=\"int32\", padding=\"post\", value= n_token - 1)\r\n",
        "\r\n",
        "    # Pad Tags (y var) and convert it into one hot encoding\r\n",
        "    tags = data_group[\"Tag_idx\"].tolist()\r\n",
        "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype=\"int32\", padding=\"post\", value= tag2idx[\"O\"])\r\n",
        "    n_tags = len(tag2idx)\r\n",
        "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\r\n",
        "    \r\n",
        "    # Split train, test and validation set\r\n",
        "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\r\n",
        "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size=0.25,train_size=0.75, random_state=2020)\r\n",
        "\r\n",
        "    print(\r\n",
        "        \"train_tokens length:\", len(train_tokens),\r\n",
        "        \"\\ntrain_tokens length:\", len(train_tokens),\r\n",
        "        \"\\ntest_tokens length:\", len(test_tokens),\r\n",
        "        \"\\ntest_tags:\", len(test_tags),\r\n",
        "        \"\\nval_tokens:\", len(val_tokens),\r\n",
        "        \"\\nval_tags:\", len(val_tags),\r\n",
        "    )\r\n",
        "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\r\n",
        "\r\n",
        "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_tokens length: 32372 \n",
            "train_tokens length: 32372 \n",
            "test_tokens length: 4796 \n",
            "test_tags: 4796 \n",
            "val_tokens: 10791 \n",
            "val_tags: 10791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK_GhcLarY62"
      },
      "source": [
        "from numpy.random import seed\r\n",
        "from numpy import array\r\n",
        "from tensorflow.random import set_seed\r\n",
        "from tensorflow.keras import Sequential, Model, Input\r\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# It's always best to set seed for reproducibility.\r\n",
        "seed(1)\r\n",
        "set_seed(2)\r\n",
        "\r\n",
        "\r\n",
        "class LSTMModel:\r\n",
        "  def __init__(self, input_dim, output_dim, input_length, n_tags, batch_size=1000, verbose=1, epochs=1, validation_split=0.2):\r\n",
        "    self.input_dim = input_dim\r\n",
        "    self.output_dim = output_dim\r\n",
        "    self.input_length = input_length\r\n",
        "    self.n_tags = n_tags\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.verbose = verbose\r\n",
        "    self.epochs = epochs\r\n",
        "    self.validation_split = validation_split\r\n",
        "\r\n",
        "  def build_model(self):\r\n",
        "    model = Sequential()\r\n",
        "\r\n",
        "    # Add Embedding layer\r\n",
        "    model.add(Embedding(input_dim=self.input_dim, output_dim=self.output_dim, input_length=self.input_length))\r\n",
        "\r\n",
        "    # Add bidirectional LSTM\r\n",
        "    model.add(Bidirectional(LSTM(units=self.output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode=\"concat\"))\r\n",
        "\r\n",
        "    # Add LSTM\r\n",
        "    model.add(LSTM(units=self.output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\r\n",
        "\r\n",
        "    # Add timeDistributed Layer\r\n",
        "    model.add(TimeDistributed(Dense(self.n_tags, activation=\"relu\")))\r\n",
        "\r\n",
        "    #Optimiser \r\n",
        "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\r\n",
        "\r\n",
        "    # Compile model\r\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n",
        "    model.summary()\r\n",
        "    \r\n",
        "    return model\r\n",
        "\r\n",
        "  def train(self, X, y, loss_chart=True):\r\n",
        "    loss = []\r\n",
        "    model = self.build_model()\r\n",
        "    for i in range(25):\r\n",
        "        # fit model for one epoch on this sequence\r\n",
        "        hist = model.fit(X, y, batch_size=self.batch_size, verbose=self.verbose, epochs=self.epochs, validation_split=self.validation_split)\r\n",
        "        loss.append(hist.history[\"loss\"][0])\r\n",
        "    if loss_chart:\r\n",
        "      results = pd.DataFrame()\r\n",
        "      results[\"with_add_lstm\"] = loss\r\n",
        "      plt.plot(results['with_add_lstm'])\r\n",
        "      plt.xlabel('Epochs')\r\n",
        "      plt.ylabel('Loss')\r\n",
        "    return loss"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uJQfqZLVGC1R",
        "outputId": "1c902427-e39d-4e8f-d06a-c0d2b9b73652"
      },
      "source": [
        "input_dim = len(list(set(data['Word'].to_list())))+1\r\n",
        "output_dim = 64\r\n",
        "input_length = max([len(s) for s in data_group[\"Word_idx\"].tolist()])\r\n",
        "n_tags = len(tag2idx)\r\n",
        "print(\"input_dim: \", input_dim, \"\\noutput_dim: \", output_dim, \"\\ninput_length: \", input_length, \"\\nn_tags: \", n_tags)\r\n",
        "\r\n",
        "model = LSTMModel(input_dim, output_dim, input_length, n_tags, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\r\n",
        "loss = model.train(train_tokens, array(train_tags), loss_chart=True)\r\n",
        "# plot_model(model)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_dim:  35179 \n",
            "output_dim:  64 \n",
            "input_length:  104 \n",
            "n_tags:  17\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 104, 64)           2251456   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 104, 128)          66048     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 104, 64)           49408     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 104, 17)           1105      \n",
            "=================================================================\n",
            "Total params: 2,368,017\n",
            "Trainable params: 2,368,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "26/26 [==============================] - 37s 1s/step - loss: 1.1744 - accuracy: 0.8312 - val_loss: 0.3234 - val_accuracy: 0.9681\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.3350 - accuracy: 0.9677 - val_loss: 0.3078 - val_accuracy: 0.9681\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.3139 - accuracy: 0.9677 - val_loss: 0.2943 - val_accuracy: 0.9681\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.3024 - accuracy: 0.9677 - val_loss: 0.2884 - val_accuracy: 0.9681\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.2951 - accuracy: 0.9677 - val_loss: 0.2848 - val_accuracy: 0.9681\n",
            "26/26 [==============================] - 27s 1s/step - loss: 0.2868 - accuracy: 0.9678 - val_loss: 0.2821 - val_accuracy: 0.9682\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.2792 - accuracy: 0.9678 - val_loss: 0.2653 - val_accuracy: 0.9682\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.2555 - accuracy: 0.9679 - val_loss: 0.2521 - val_accuracy: 0.9683\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.2458 - accuracy: 0.9680 - val_loss: 0.2383 - val_accuracy: 0.9684\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.2353 - accuracy: 0.9682 - val_loss: 0.2341 - val_accuracy: 0.9685\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1993 - accuracy: 0.9685 - val_loss: 0.1939 - val_accuracy: 0.9689\n",
            "26/26 [==============================] - 30s 1s/step - loss: 0.1621 - accuracy: 0.9690 - val_loss: 0.1481 - val_accuracy: 0.9695\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1416 - accuracy: 0.9692 - val_loss: 0.1427 - val_accuracy: 0.9697\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.1346 - accuracy: 0.9695 - val_loss: 0.1417 - val_accuracy: 0.9698\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.1305 - accuracy: 0.9697 - val_loss: 0.1379 - val_accuracy: 0.9696\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1269 - accuracy: 0.9697 - val_loss: 0.1350 - val_accuracy: 0.9702\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1232 - accuracy: 0.9703 - val_loss: 0.1329 - val_accuracy: 0.9708\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1205 - accuracy: 0.9711 - val_loss: 0.1342 - val_accuracy: 0.9712\n",
            "26/26 [==============================] - 30s 1s/step - loss: 0.1174 - accuracy: 0.9717 - val_loss: 0.1306 - val_accuracy: 0.9722\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1241 - accuracy: 0.9720 - val_loss: 0.1378 - val_accuracy: 0.9715\n",
            "26/26 [==============================] - 29s 1s/step - loss: 0.1211 - accuracy: 0.9716 - val_loss: 0.1307 - val_accuracy: 0.9723\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1156 - accuracy: 0.9730 - val_loss: 0.1288 - val_accuracy: 0.9721\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1131 - accuracy: 0.9730 - val_loss: 0.1275 - val_accuracy: 0.9733\n",
            "26/26 [==============================] - 28s 1s/step - loss: 0.1104 - accuracy: 0.9740 - val_loss: 0.1304 - val_accuracy: 0.9732\n",
            "26/26 [==============================] - 30s 1s/step - loss: 0.0975 - accuracy: 0.9749 - val_loss: 0.1058 - val_accuracy: 0.9748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}